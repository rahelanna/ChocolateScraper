name: Scrapy Crawl

on:
  schedule:
    - cron: '0 0 * * *' # run at midnight
  workflow_dispatch: # can be run manually

jobs:
  scrapy:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres
        env:
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_HOST_AUTH_METHOD: trust
          # POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB }}

        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.12.5

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Check running Docker containers
      run: |
        echo "Listing running Docker containers:"
        docker ps -a
        echo "Checking open ports:"
        netstat -tuln


    - name: Run Scrapy spider
      env:
        POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
        POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
        POSTGRES_DB: ${{ secrets.POSTGRES_DB }}
        POSTGRES_HOST: localhost
      run:
        scrapy crawl chocolatespider
